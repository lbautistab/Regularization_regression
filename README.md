# ğŸ“‰ **RegularizaciÃ³n en RegresiÃ³n: Domina el Sesgo-Varianza**

ğŸ”¬ *â€œLa simplicidad no es una meta. Es el subproducto de una buena idea y expectativas modestas.â€ â€“ Paul Rand*

Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)

## ğŸš€  **MÃ©todos de regularizaciÃ³n**

1. **Ridge Regression (L2)**

Para la estimaciÃ³n de los coeficientes en mÃ­nimos cuadrados debemos minimizar la suma de los errores al cuadrado. Para generar una regresiÃ³n tipo rigde agregamos la penalizaciÃ³n y mÃ­nimizamos la expresiÃ³n:

$$\begin{equation}
\sum_{i=1}^n{(y_i - \beta_o - \sum_{j=1}^p{\beta_jx_{ij}})^2} + \lambda\sum_{j=1}^p{\beta_{j}^2}
\end{equation}$$

Donde la primera expresiÃ³n es la suma de los errores al cuadrado y $$\lambda$$ es un parÃ¡metro que debe ser tuneado.

2. **Lasso Regression (L1)**

A diferencia de Ridge, matemÃ¡ticamente el Ãºnico cambio es que ahora los coeficientes de la penalizaciÃ³n estÃ¡n en valor absoluto en vez de elevados al cuadrado. Esto tiene efectos distintos a la Ridge:  

- No penaliza de la misma manera a los coeficientes muy grandes.
- En la regresiÃ³n Ridge los coeficientes tienden hacia cero, en la regresiÃ³n Lasso los coeficientes puede volverse cero, lo que implica que la regresiÃ³n Lasso tiene otro efecto y es que automÃ¡ticamente depura las variables que no agregan poder predictivo al modelo.

$$\begin{equation}
\sum_{i=1}^n{(y_i - \beta_o - \sum_{j=1}^p{\beta_jx_{ij}})^2} + \lambda\sum_{j=1}^p{|\beta_{j}|}
\end{equation}$$

3. **Elastic Net**

Es una combinaciÃ³n de Ridge y Lasso. Se decide entonces quÃ© peso se le da a cada mÃ©todo de penalizaciÃ³n y se implementa la regresiÃ³n:

$$\begin{equation}
\sum_{i=1}^n{(y_i - \beta_o - \sum_{j=1}^p{\beta_jx_{ij}})^2} + \lambda_{1}\sum_{j=1}^p{\beta_{j}^2} +\lambda_{2}\sum_{j=1}^p{|\beta_{j}|}
\end{equation}$$


## ğŸ¯ **Â¿Por quÃ© es importante?**

En un mundo lleno de datos, mÃ¡s variables no siempre significan mejores modelos. La regularizaciÃ³n es una de las herramientas mÃ¡s poderosas para:

- Controlar el sobreajuste
- Mejorar la interpretabilidad del modelo
- Aumentar la robustez predictiva
- Seleccionar variables automÃ¡ticamente (Lasso)
